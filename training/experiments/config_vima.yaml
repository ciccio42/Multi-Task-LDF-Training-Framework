debug: False
EXPERT_DATA:    null 
save_path:      null
wandb_log: True 
project_name: None
log_freq: 1
val_freq: 1
print_freq: 1
save_freq: 1
save_optim:     True
resume:         False
resume_path:    null
resume_step:    -1
device:         0
exp_name:       -1
epochs:         10
bsize:          30
vsize:          ${bsize} # optionally, set to a different batchsize for validation loader
optimizer:      null #Adam # RMSProp
task_names: null
dataset_target: 'multi_task_il.datasets.multi_task_datasets.MultiTaskPairedDataset'
# imitation loss multipliers
bc_mul:         1

train_cfg:
  batch_size:           ${bsize}
  val_size:             ${vsize}
  lr:                   1e-3 #5e-4
  optimizer:            ${optimizer}
  epochs:               ${epochs}
  log_freq:             ${log_freq}
  save_freq:            ${save_freq}
  print_freq:           ${print_freq}
  bc_loss_mult:         ${bc_mul}
  dataset:              ${dataset_cfg}
  sampler:              ${samplers}
  target_update_freq:   5  # used for soft parameter update
  early_stopping: ${early_stopping_cfg}
  weight_decay:  0.05
  lr_schedule: 'ReduceLROnPlateau' # ExponentialDecay

# early stopping configurator
early_stopping_cfg:
  patience: 10
  delta: 1e-3

# tasks related
tasks:       ???
single_task: False 
exclude_task: False 
use_all_tasks: False 
set_same_n:   -1 
limit_num_traj: -1
limit_num_demo: -1
defaults:
  - tasks_cfgs:  7_tasks

# dataset/loader/samplers
loader_workers:   20
samplers:        
  batch_size:         ${bsize}
  drop_last:          False
  shuffle:            True 
  # Policy 1: At each slot is assigned a RandomSampler
  balancing_policy: 1 # 0, 1, 2 
val_skip:             False # whether to let the samplers skip some **sub**tasks
train_skip:           True

dataset_cfg:
  _target_:         ${dataset_target}
  root_dir:         ${EXPERT_DATA}/
  mode:             ???
  tasks_spec:       ${tasks}
  aux_pose:         True # only required for DAML baseline 
  height:           100
  width:            180
  split:            [0.9, 0.1]
  demo_T:           4
  obs_T:            7
  normalize_action: True
  # x, y, z, e_x, e_y, e_z
  normalization_ranges: [[-0.35,  0.25],
                        [-0.30,  0.30],
                        [ 0.60,  1.20],
                        [-3.14,  3.14911766],
                        [-3.14911766, 3.14911766],
                        [-3.14911766,  3.14911766]]
  n_action_bin: 256
  take_first_frame: False
  data_augs:        ${augs}
  non_sequential:   False
  crop_twice:       False
  state_spec:       [ee_aa, gripper_qpos]
  allow_val_skip:   ${val_skip}
  allow_train_skip: ${train_skip}
  use_strong_augs:  True 
  select_random_frames: True
  compute_obj_distribution: False
  agent_name: 'ur5e'
  demo_name: 'panda'

augs:
  strong_jitter:     0.01    
  weak_jitter:       0.01
  grayscale:         0.01   
  blur:              [0.1, 2.0] 
  flip:              0.05   # only flip for the strong augs
  weak_crop_scale:   [0.8, 1.0] 
  strong_crop_scale: [0.6, 1.0] 
  weak_crop_ratio:   [1.8, 1.8]
  strong_crop_ratio: [1.8, 1.8]
  use_affine:        False  
  rand_trans:        0.1


# model related
policy: ???  # set to any set of configs via command line, e.g. policy='${tosil}'

actions:
  n_layers:         2    
  out_dim:          64
  hidden_dim:       128
  adim:             8
  n_mixtures:       2   
  const_var:        False
  sep_var:          True 
  concat_demo_head:   False
  concat_demo_act:    True
  demo_mean:        ${policy.demo_mean}

attn:
  embed_hidden:     256
  dropout:          0.2
  n_attn_layers:    2
  attn_heads:       4
  attn_ff:          128
  pos_enc:          True 
  fuse_starts:      0   # default 0 so attend everywhere
  causal:           True
  attend_demo:      True
  demo_out:         True     
  img_cfg:           
    network_flag:   0 # e.g Res18  
    out_feature:    256
    drop_dim:       3
    pretrained:     True 
  

vima:
  _target_: multi_task_il.models.vima.policy.Policy
  embed_dim: 640
  xf_n_layers: 7
  sattn_n_heads: 20
  xattn_n_heads: 20
  views: ['front']
  return_dist: True
  concat_state: True